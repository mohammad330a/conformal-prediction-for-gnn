{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#GNN"
      ],
      "metadata": {
        "id": "1Kn8c9hLrkMH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xZXwL695tBv",
        "outputId": "2fe28f44-9caa-480e-efe9-14327f269e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.4.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_version = str(torch.__version__)\n",
        "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "!pip install torch-scatter -f $scatter_src\n",
        "!pip install torch-sparse -f $sparse_src\n",
        "!pip install torch-geometric\n",
        "!pip install ogb\n",
        "!pip install fsspec==2024.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piHfsa235z9Y",
        "outputId": "e10f1f1b-a226-446e-c3b0-a263d2e6ef01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_scatter-2.1.2%2Bpt24cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt24cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_sparse-0.6.18%2Bpt24cu121-cp310-cp310-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt24cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.3.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (71.0.4)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n",
            "Collecting fsspec==2024.3.1\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2024.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "dataset_name = 'ogbn-arxiv'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                transform=T.ToSparseTensor())\n",
        "print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
        "\n",
        "data = dataset[0]\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0GGDLN07-Rw",
        "outputId": "fba797b0-6bfe-43b1-fa1c-5fdf93e90c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:02<00:00, 38.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1640.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 690.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n",
            "/usr/local/lib/python3.10/dist-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ogbn-arxiv dataset has 1 graph\n",
            "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The graph has {} features'.format(data.num_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vts1LDo8U9J",
        "outputId": "12b8f6ea-f2bb-4b4f-f974-31568d8039b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The graph has 128 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ],
      "metadata": {
        "id": "qgMBti7G8fK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'ogbn-arxiv'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "data = data.to(device)\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train'].to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOe2hg708mTS",
        "outputId": "dd36309b-1caa-4cee-bbd8-87420e934e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
            "/usr/local/lib/python3.10/dist-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in split_idx:\n",
        "  print(s, len(split_idx[s]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8I4uRwU8vio",
        "outputId": "fc0c5382-c455-45ee-e176-727f45ceed39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 90941\n",
            "valid 29799\n",
            "test 48603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList(\n",
        "            [GCNConv(in_channels=input_dim, out_channels=hidden_dim)] +\n",
        "            [GCNConv(in_channels=hidden_dim, out_channels=hidden_dim) for _ in range(num_layers - 2)] +\n",
        "            [GCNConv(in_channels=hidden_dim, out_channels=output_dim)]\n",
        "        )\n",
        "        self.bns = torch.nn.ModuleList(\n",
        "            [torch.nn.BatchNorm1d(num_features=hidden_dim) for _ in range(num_layers - 1)]\n",
        "        )\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "        self.dropout = dropout\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for conv, bn in zip(self.convs[:-1], self.bns):\n",
        "            x = F.relu(bn(conv(x, adj_t)))\n",
        "            if self.training:\n",
        "                x = F.dropout(x, p=self.dropout)\n",
        "            x = x\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        out = x if self.return_embeds else self.softmax(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1ao-sKfM9ZI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t)\n",
        "    loss = loss_fn(out[train_idx], data.y[train_idx].reshape(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "m6MF4WGlCPcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(data.x, data.adj_t)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ],
      "metadata": {
        "id": "1taeTQhFM0jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 3,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 100,\n",
        "}\n",
        "args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfEMOGhgNEWW",
        "outputId": "d76d88d9-5115-47fc-c8ac-367cc1ba9d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'device': 'cpu',\n",
              " 'num_layers': 3,\n",
              " 'hidden_dim': 256,\n",
              " 'dropout': 0.5,\n",
              " 'lr': 0.01,\n",
              " 'epochs': 100}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(data.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ],
      "metadata": {
        "id": "IHJ_SO6QNJOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, data, train_idx, optimizer, loss_fn)\n",
        "  result = test(model, data, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EAVtroZNoXP",
        "outputId": "7463ea29-09ce-430a-9bad-0e358ca61784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_sparse/tensor.py:574: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  return torch.sparse_csr_tensor(rowptr, col, value, self.sizes())\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: -0.0243, Train: 11.00%, Valid: 22.97% Test: 21.56%\n",
            "Epoch: 02, Loss: -0.2669, Train: 36.22%, Valid: 36.05% Test: 33.64%\n",
            "Epoch: 03, Loss: -0.3813, Train: 28.08%, Valid: 21.08% Test: 24.13%\n",
            "Epoch: 04, Loss: -0.4332, Train: 29.18%, Valid: 16.59% Test: 14.87%\n",
            "Epoch: 05, Loss: -0.4598, Train: 32.83%, Valid: 18.37% Test: 14.83%\n",
            "Epoch: 06, Loss: -0.4745, Train: 41.34%, Valid: 36.28% Test: 32.46%\n",
            "Epoch: 07, Loss: -0.4873, Train: 45.36%, Valid: 42.56% Test: 39.60%\n",
            "Epoch: 08, Loss: -0.4965, Train: 46.50%, Valid: 44.81% Test: 43.51%\n",
            "Epoch: 09, Loss: -0.5086, Train: 46.63%, Valid: 45.27% Test: 44.57%\n",
            "Epoch: 10, Loss: -0.5230, Train: 47.26%, Valid: 46.56% Test: 45.52%\n",
            "Epoch: 11, Loss: -0.5366, Train: 47.49%, Valid: 47.27% Test: 46.54%\n",
            "Epoch: 12, Loss: -0.5411, Train: 48.14%, Valid: 50.69% Test: 51.23%\n",
            "Epoch: 13, Loss: -0.5440, Train: 48.93%, Valid: 53.35% Test: 55.35%\n",
            "Epoch: 14, Loss: -0.5486, Train: 49.45%, Valid: 53.89% Test: 56.76%\n",
            "Epoch: 15, Loss: -0.5537, Train: 49.46%, Valid: 52.86% Test: 55.84%\n",
            "Epoch: 16, Loss: -0.5572, Train: 49.81%, Valid: 52.61% Test: 55.57%\n",
            "Epoch: 17, Loss: -0.5608, Train: 50.99%, Valid: 53.90% Test: 56.48%\n",
            "Epoch: 18, Loss: -0.5644, Train: 52.53%, Valid: 55.50% Test: 57.61%\n",
            "Epoch: 19, Loss: -0.5691, Train: 54.49%, Valid: 57.08% Test: 58.70%\n",
            "Epoch: 20, Loss: -0.5712, Train: 55.94%, Valid: 58.22% Test: 59.33%\n",
            "Epoch: 21, Loss: -0.5741, Train: 56.86%, Valid: 59.30% Test: 60.69%\n",
            "Epoch: 22, Loss: -0.5760, Train: 57.12%, Valid: 59.87% Test: 61.53%\n",
            "Epoch: 23, Loss: -0.5780, Train: 56.98%, Valid: 59.50% Test: 61.61%\n",
            "Epoch: 24, Loss: -0.5801, Train: 56.58%, Valid: 58.67% Test: 61.14%\n",
            "Epoch: 25, Loss: -0.5814, Train: 56.60%, Valid: 58.36% Test: 60.74%\n",
            "Epoch: 26, Loss: -0.5829, Train: 57.05%, Valid: 58.76% Test: 61.09%\n",
            "Epoch: 27, Loss: -0.5839, Train: 57.70%, Valid: 59.69% Test: 61.75%\n",
            "Epoch: 28, Loss: -0.5844, Train: 58.31%, Valid: 60.59% Test: 62.09%\n",
            "Epoch: 29, Loss: -0.5862, Train: 58.54%, Valid: 60.88% Test: 61.82%\n",
            "Epoch: 30, Loss: -0.5876, Train: 58.58%, Valid: 60.90% Test: 61.31%\n",
            "Epoch: 31, Loss: -0.5873, Train: 58.58%, Valid: 60.88% Test: 61.15%\n",
            "Epoch: 32, Loss: -0.5879, Train: 58.64%, Valid: 60.91% Test: 61.16%\n",
            "Epoch: 33, Loss: -0.5889, Train: 58.83%, Valid: 60.97% Test: 61.03%\n",
            "Epoch: 34, Loss: -0.5897, Train: 59.03%, Valid: 61.06% Test: 60.59%\n",
            "Epoch: 35, Loss: -0.5902, Train: 59.20%, Valid: 61.10% Test: 60.44%\n",
            "Epoch: 36, Loss: -0.5907, Train: 59.32%, Valid: 61.45% Test: 61.08%\n",
            "Epoch: 37, Loss: -0.5925, Train: 59.44%, Valid: 61.87% Test: 61.88%\n",
            "Epoch: 38, Loss: -0.5927, Train: 59.42%, Valid: 62.13% Test: 62.51%\n",
            "Epoch: 39, Loss: -0.5935, Train: 59.29%, Valid: 62.11% Test: 62.81%\n",
            "Epoch: 40, Loss: -0.5933, Train: 59.18%, Valid: 62.02% Test: 62.84%\n",
            "Epoch: 41, Loss: -0.5935, Train: 59.22%, Valid: 61.84% Test: 62.54%\n",
            "Epoch: 42, Loss: -0.5943, Train: 59.33%, Valid: 61.72% Test: 62.36%\n",
            "Epoch: 43, Loss: -0.5946, Train: 59.51%, Valid: 62.05% Test: 62.70%\n",
            "Epoch: 44, Loss: -0.5952, Train: 59.74%, Valid: 62.28% Test: 63.01%\n",
            "Epoch: 45, Loss: -0.5949, Train: 59.90%, Valid: 62.45% Test: 63.01%\n",
            "Epoch: 46, Loss: -0.5960, Train: 59.95%, Valid: 62.32% Test: 62.43%\n",
            "Epoch: 47, Loss: -0.5967, Train: 59.85%, Valid: 61.79% Test: 61.50%\n",
            "Epoch: 48, Loss: -0.5968, Train: 59.90%, Valid: 61.70% Test: 61.08%\n",
            "Epoch: 49, Loss: -0.5969, Train: 60.08%, Valid: 62.15% Test: 61.71%\n",
            "Epoch: 50, Loss: -0.5974, Train: 60.17%, Valid: 62.59% Test: 62.69%\n",
            "Epoch: 51, Loss: -0.5980, Train: 60.05%, Valid: 62.60% Test: 62.99%\n",
            "Epoch: 52, Loss: -0.5981, Train: 60.11%, Valid: 62.66% Test: 63.01%\n",
            "Epoch: 53, Loss: -0.5984, Train: 60.16%, Valid: 62.33% Test: 62.40%\n",
            "Epoch: 54, Loss: -0.5987, Train: 60.09%, Valid: 62.27% Test: 62.39%\n",
            "Epoch: 55, Loss: -0.5992, Train: 60.15%, Valid: 62.52% Test: 62.59%\n",
            "Epoch: 56, Loss: -0.5997, Train: 60.28%, Valid: 62.74% Test: 63.15%\n",
            "Epoch: 57, Loss: -0.6006, Train: 60.41%, Valid: 62.72% Test: 62.99%\n",
            "Epoch: 58, Loss: -0.6006, Train: 60.70%, Valid: 62.79% Test: 62.56%\n",
            "Epoch: 59, Loss: -0.6007, Train: 60.62%, Valid: 62.58% Test: 62.08%\n",
            "Epoch: 60, Loss: -0.6012, Train: 60.61%, Valid: 62.81% Test: 62.91%\n",
            "Epoch: 61, Loss: -0.6009, Train: 60.26%, Valid: 62.57% Test: 63.57%\n",
            "Epoch: 62, Loss: -0.6015, Train: 60.28%, Valid: 62.52% Test: 63.57%\n",
            "Epoch: 63, Loss: -0.6019, Train: 60.28%, Valid: 62.50% Test: 63.18%\n",
            "Epoch: 64, Loss: -0.6025, Train: 60.28%, Valid: 62.47% Test: 63.09%\n",
            "Epoch: 65, Loss: -0.6020, Train: 60.54%, Valid: 62.70% Test: 63.63%\n",
            "Epoch: 66, Loss: -0.6026, Train: 60.58%, Valid: 62.64% Test: 63.61%\n",
            "Epoch: 67, Loss: -0.6034, Train: 60.79%, Valid: 62.71% Test: 63.19%\n",
            "Epoch: 68, Loss: -0.6029, Train: 60.68%, Valid: 62.41% Test: 62.34%\n",
            "Epoch: 69, Loss: -0.6031, Train: 60.59%, Valid: 62.20% Test: 62.21%\n",
            "Epoch: 70, Loss: -0.6037, Train: 60.67%, Valid: 62.51% Test: 62.61%\n",
            "Epoch: 71, Loss: -0.6042, Train: 60.97%, Valid: 62.80% Test: 62.88%\n",
            "Epoch: 72, Loss: -0.6049, Train: 61.04%, Valid: 62.85% Test: 62.73%\n",
            "Epoch: 73, Loss: -0.6047, Train: 61.18%, Valid: 62.81% Test: 62.36%\n",
            "Epoch: 74, Loss: -0.6063, Train: 61.42%, Valid: 63.21% Test: 63.08%\n",
            "Epoch: 75, Loss: -0.6085, Train: 61.59%, Valid: 63.21% Test: 63.17%\n",
            "Epoch: 76, Loss: -0.6118, Train: 61.48%, Valid: 62.98% Test: 63.26%\n",
            "Epoch: 77, Loss: -0.6120, Train: 61.73%, Valid: 63.09% Test: 62.95%\n",
            "Epoch: 78, Loss: -0.6135, Train: 61.67%, Valid: 62.50% Test: 61.42%\n",
            "Epoch: 79, Loss: -0.6149, Train: 61.83%, Valid: 62.93% Test: 62.03%\n",
            "Epoch: 80, Loss: -0.6158, Train: 61.98%, Valid: 63.36% Test: 63.55%\n",
            "Epoch: 81, Loss: -0.6157, Train: 61.51%, Valid: 62.98% Test: 62.76%\n",
            "Epoch: 82, Loss: -0.6171, Train: 60.58%, Valid: 62.31% Test: 62.08%\n",
            "Epoch: 83, Loss: -0.6178, Train: 60.12%, Valid: 61.83% Test: 61.95%\n",
            "Epoch: 84, Loss: -0.6194, Train: 60.21%, Valid: 62.21% Test: 63.00%\n",
            "Epoch: 85, Loss: -0.6194, Train: 60.52%, Valid: 62.66% Test: 63.18%\n",
            "Epoch: 86, Loss: -0.6207, Train: 61.36%, Valid: 63.10% Test: 63.21%\n",
            "Epoch: 87, Loss: -0.6215, Train: 62.02%, Valid: 63.83% Test: 64.59%\n",
            "Epoch: 88, Loss: -0.6213, Train: 62.26%, Valid: 64.03% Test: 64.54%\n",
            "Epoch: 89, Loss: -0.6227, Train: 62.46%, Valid: 64.17% Test: 64.21%\n",
            "Epoch: 90, Loss: -0.6224, Train: 62.63%, Valid: 64.06% Test: 64.00%\n",
            "Epoch: 91, Loss: -0.6223, Train: 62.39%, Valid: 63.97% Test: 64.77%\n",
            "Epoch: 92, Loss: -0.6226, Train: 62.58%, Valid: 64.23% Test: 64.98%\n",
            "Epoch: 93, Loss: -0.6236, Train: 62.28%, Valid: 63.12% Test: 62.50%\n",
            "Epoch: 94, Loss: -0.6240, Train: 62.74%, Valid: 63.90% Test: 63.87%\n",
            "Epoch: 95, Loss: -0.6245, Train: 61.98%, Valid: 62.91% Test: 63.51%\n",
            "Epoch: 96, Loss: -0.6249, Train: 62.01%, Valid: 63.44% Test: 64.21%\n",
            "Epoch: 97, Loss: -0.6258, Train: 62.70%, Valid: 64.21% Test: 64.52%\n",
            "Epoch: 98, Loss: -0.6261, Train: 62.49%, Valid: 63.87% Test: 64.70%\n",
            "Epoch: 99, Loss: -0.6259, Train: 62.21%, Valid: 63.24% Test: 64.33%\n",
            "Epoch: 100, Loss: -0.6259, Train: 62.91%, Valid: 64.06% Test: 64.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_result = test(best_model, data, split_idx, evaluator)\n",
        "train_acc, valid_acc, test_acc = best_result\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zagA-7ukNvjG",
        "outputId": "d3b4af9f-19ef-4e4c-c4ee-3e24875feb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 62.58%, Valid: 64.23% Test: 64.98%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conformal Prediction"
      ],
      "metadata": {
        "id": "-CejQKrfxbTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "since we didn't use labels of the nodes in validation set during training of the model and we didn't change the model based on validation set at all, we will use validation set as calibration set in this implementation"
      ],
      "metadata": {
        "id": "PpHvX4_pxeyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###initializing calibration set"
      ],
      "metadata": {
        "id": "T3cSlqkGp082"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = best_model(data.x, data.adj_t)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QItA79OZy_xO",
        "outputId": "d432a17e-4989-4e2a-9fa1-b3ffb16781c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([169343, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calib_idx = split_idx['valid']\n",
        "out[calib_idx].shape"
      ],
      "metadata": {
        "id": "n_AVeiMZ4BnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df68e31-e294-4042-94a4-d7aa8dabdfd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([29799, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.y.min())\n",
        "print(data.y.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1uxujfl4L0R",
        "outputId": "b9ee4cbc-c772-4910-b46a-2caf6dd7748d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0)\n",
            "tensor(39)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.y[calib_idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjFXjd_L35BE",
        "outputId": "12cdb60a-635f-499c-f211-cd556c2143b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 8],\n",
              "        [19],\n",
              "        [16],\n",
              "        ...,\n",
              "        [30],\n",
              "        [16],\n",
              "        [28]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conformal Prediction Functions"
      ],
      "metadata": {
        "id": "8NupHrSvr-_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_DAPS_scores(adj_t, base_scores, lamb, iterations):\n",
        "  num_nodes = adj_t.size(dim=0)\n",
        "  degrees = adj_t.sum(dim=1)\n",
        "  rows, cols, wieghts = adj_t.coo()\n",
        "\n",
        "  A = torch.sparse_coo_tensor(\n",
        "    torch.tensor([list(rows), list(cols)]),\n",
        "    torch.ones(len(list(rows))),\n",
        "    size=[num_nodes, num_nodes]\n",
        "  )\n",
        "\n",
        "  D_inv = torch.sparse_coo_tensor(\n",
        "    torch.tensor([[_ for _ in range(num_nodes)], [_ for _ in range(num_nodes)]]),\n",
        "    [1.0/d for d in degrees],\n",
        "    size=[num_nodes, num_nodes]\n",
        "  )\n",
        "\n",
        "  cur_scores = base_scores.clone()\n",
        "  for t in range(iterations):\n",
        "    cur_scores = (1-lamb)*base_scores + lamb*D_inv@A@cur_scores\n",
        "\n",
        "  return cur_scores"
      ],
      "metadata": {
        "id": "nY6uOxF9EMlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DAPS_scores = get_DAPS_scores(data.adj_t, out, 0.5, 1)\n",
        "DAPS_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYAue_UlLGI1",
        "outputId": "18908d19-1115-44ad-9dc8-f72a60d05c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[7.7288e-08, 6.9113e-08, 5.4967e-07,  ..., 1.4201e-07, 6.0433e-07,\n",
              "         9.7248e-08],\n",
              "        [1.7057e-09, 1.9465e-09, 8.7014e-07,  ..., 3.1826e-09, 3.8735e-09,\n",
              "         2.2287e-09],\n",
              "        [2.9829e-13, 4.3658e-13, 3.4426e-11,  ..., 1.3424e-12, 1.1977e-12,\n",
              "         3.0237e-13],\n",
              "        ...,\n",
              "        [1.2087e-09, 2.5366e-10, 1.8979e-05,  ..., 2.4484e-09, 1.0351e-06,\n",
              "         4.3633e-10],\n",
              "        [8.9329e-11, 1.0907e-10, 9.6861e-10,  ..., 1.3626e-10, 1.0433e-09,\n",
              "         1.5471e-10],\n",
              "        [1.0031e-15, 1.0356e-15, 7.5853e-14,  ..., 1.4304e-14, 4.8314e-12,\n",
              "         1.8413e-15]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calib_scores = DAPS_scores[calib_idx, list(data.y[calib_idx])]\n",
        "calib_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXjcV-8SSJBi",
        "outputId": "119403bc-93b4-47d8-aa54-118e6f12ca53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000e+00, 5.1222e-13, 8.7491e-01,  ..., 9.3728e-01, 7.6589e-04,\n",
              "        1.0000e+00], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "alpha = 0.2\n",
        "q_hat = torch.quantile(calib_scores, math.floor(alpha*(len(calib_idx) - 1))/len(calib_idx))\n",
        "q_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-epRifRT2yh",
        "outputId": "39ef0dc3-34a1-4ef5-d53b-ab24cdc06d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.1208e-07, grad_fn=<SqueezeBackward4>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(calib_scores >= q_hat).sum()/calib_scores.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0solZRYT-pD",
        "outputId": "465b514e-7c57-4d66-dbd8-bdd131c81660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8000)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conformal_labels = (DAPS_scores >= q_hat)"
      ],
      "metadata": {
        "id": "h4pTNWHuWyM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"the average of sizes of the sets is: {conformal_labels.sum(dim=1).float().mean()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmHhcGjTb6Xy",
        "outputId": "1b3c19fb-645b-4877-b1fc-95634b5db145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the average of sizes of the sets is: 5.477498531341553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node = 10\n",
        "conformal_labels[node].nonzero()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lneMBvjyZ-Np",
        "outputId": "d40c22f6-211d-4202-a411-4c78975b2792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[24],\n",
              "        [28],\n",
              "        [36]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}